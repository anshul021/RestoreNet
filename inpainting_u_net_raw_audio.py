# -*- coding: utf-8 -*-
"""inpainting_U-Net_raw_audio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vf0AltqHRfVFVOu1peW7ZlWIVBVe_8rm

### Model Architecture Summary: AudioInpaintUNet

The `AudioInpaintUNet` is a U-Net style encoder-decoder network designed for stereo audio inpainting.

**Input:**
*   Takes a 3-channel input: `[Left_Channel, Right_Channel, Mask]`, where the masked regions in the audio channels are zeroed out, and the mask channel indicates the location of the hole (1 inside the hole, 0 elsewhere). Shape: `[B, 3, T]` (Batch size, Channels, Time/Length).

**Output:**
*   Outputs a 2-channel tensor representing the `residual` to be added back to the masked input to restore the audio. Shape: `[B, 2, T]`.
*   It also returns the `restored` audio by adding the residual to the masked input within the hole region if `use_residual_composition` is True (which is the default).

**Key Components:**

1.  **Encoder:**
    *   Consists of a series of `ConvBlock1d` layers.
    *   Each stage downsamples the input by a factor of 2 using a stride of 2 in the first `ConvBlock1d`.
    *   Followed by another `ConvBlock1d` at the same resolution for refinement.
    *   Generates feature maps at progressively lower resolutions and higher channel counts.
    *   Skip connections are created at each encoder stage to be concatenated with the corresponding decoder stage.

2.  **Bottleneck:**
    *   A `DilatedBottleneck` module is used at the deepest point of the U-Net.
    *   This module uses several `ConvBlock1d` layers with increasing dilation rates to expand the receptive field and integrate information over a larger context without further downsampling.

3.  **Decoder:**
    *   Consists of a series of `UpConv1d` and `ConvBlock1d` layers.
    *   `UpConv1d` uses `ConvTranspose1d` to upsample the feature maps, typically doubling the resolution.
    *   The upsampled feature maps are concatenated with the skip connection from the corresponding encoder stage.
    *   Followed by two `ConvBlock1d` layers (`dec_post`) to refine the features after the skip connection.

4.  **Final Output Layer:**
    *   A simple `nn.Conv1d` layer projects the output of the final decoder stage (which has `base_ch` channels) down to 2 channels (stereo).
    *   The weights and biases of this layer are initialized to zeros, ensuring that the initial output is a zero residual, effectively returning the masked input before any training.

**Building Blocks:**

*   **`_norm`:** A helper function that provides `GroupNorm` for normalization, chosen for stability with smaller batch sizes.
*   **`ConvBlock1d`:** A standard convolutional block including `nn.Conv1d`, `GroupNorm`, `PReLU` activation (or `Identity` if `act` is False), and optional `Dropout`.
*   **`UpConv1d`:** An upsampling block using `nn.ConvTranspose1d`, followed by `GroupNorm` and `PReLU`.
*   **`DilatedBottleneck`:** A sequence of `ConvBlock1d` layers with exponentially increasing dilation rates.

**Residual Composition:**

*   The model primarily learns a `residual` signal.
*   The final `restored` output is typically created by adding this learned residual to the original masked input, but only within the masked region (`restored = x_masked_stereo + residual * mask`). This ensures that the model only modifies the parts of the audio that were originally zeroed out.

**Parameters:**

*   `in_ch`: Input channels (default 3).
*   `out_ch`: Output channels (default 2).
*   `base_ch`: Number of channels in the first encoder stage (default 64).
*   `depth`: Number of encoder/decoder stages (default 3).
*   `bottleneck_layers`: Number of dilated convolutional layers in the bottleneck (default 2).
*   `use_residual_composition`: Whether to add the residual to the masked input or treat the residual as the direct output.
*   `dropout_rate`: Rate for dropout layers within `ConvBlock1d`.
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
SAMPLE_RATE = 44100
BASE_CHANNELS = 32 # previously 64
U_NET_DEPTH = 6 # previously 8, causing loss of information?
BOTTLENECK_LAYERS = 3 # previously 4, changed to 4 becase of speculation that too much information was being lost
WIN_LEN = 65536 # previously 262144
MILLISECONDS_TO_SECONDS = 1e-3
MIN_HOLE_LENGTH = 5*MILLISECONDS_TO_SECONDS*SAMPLE_RATE
MAX_HOLE_LENGTH = 10*MILLISECONDS_TO_SECONDS*SAMPLE_RATE
BATCH_SZ = 8 # see if 16 works
NUM_WORKERS = 0 # previously 2, changed to 0 for debugging
start_LR = 3e-3
end_LR = 1e-4
WD = 1e-5 # previously 1e-3
DROPOUT = 0.2
accumulation_steps = 4
MAX_STEPS = 20000
VAL_EVERY = 250
INPUT_DIR = "/content/data"
CKPT_DIR = "drive/MyDrive/Checkpoints"
os.makedirs(CKPT_DIR, exist_ok=True)

print(len(os.listdir("/content/drive/MyDrive/Clean Audio")))
print(os.path.getsize("/content/drive/MyDrive/Clean Audio/10_10_35_42.wav.npy"))

import shutil
from tqdm.notebook import tqdm
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor

src_dir = "/content/drive/MyDrive/Clean Audio"
dst_dir = "/content/data"
os.makedirs(dst_dir, exist_ok=True)

TIMEOUT = 20  # seconds

def _copy_task(src, dst_dir):
    dst = os.path.join(dst_dir, os.path.basename(src))
    if not os.path.exists(dst):
        shutil.copy(src, dst)
    return dst

def copy_file(src):
    print("Copying", src)
    for attempt in range(1, 3):  # Try twice
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_copy_task, src, dst_dir)
            try:
                result = future.result(timeout=TIMEOUT)
                print(f"Successfully copied {src} on attempt {attempt}")
                return result
            except concurrent.futures.TimeoutError:
                print(f"Attempt {attempt}: Copying {src} exceeded {TIMEOUT} seconds")
    print(f"Failed to copy {src} after 2 attempts.")

print(os.listdir(src_dir)[:5])

len(os.listdir(dst_dir))

def clear_data():
    for item in os.listdir("/content/data"):
        item_path = os.path.join("/content/data", item)
        if os.path.isfile(item_path):
            os.remove(item_path)
        elif os.path.isdir(item_path):
            shutil.rmtree(item_path)

clear_data()

num_files = 5

import random
random.seed(42)
files = [os.path.join(src_dir, f) for f in os.listdir(src_dir) if f.endswith(".npy")]
random_files = random.sample(files, num_files)

random_files[:5]

shutil.copy("/content/drive/MyDrive/Clean Audio/6_5_16_23.wav.npy", "/content/data/6_5_16_23.wav.npy")

copy_file("/content/drive/MyDrive/Clean Audio/10_6_19_28.wav.npy")

num_files_processed = 1
for f in random_files:
    print("Copying file "+str(num_files_processed)+": "+str(f))
    copy_file(f)
    num_files_processed += 1

os.path.exists("/content/drive/MyDrive/Clean Audio/10_4_10_19.wav.npy")

with ThreadPoolExecutor(max_workers=8) as executor:
    list(tqdm(executor.map(copy_file, files), total=len(files)))

def circular_shift(y, shift_samples):
    if shift_samples == 0:
        return f
    return np.roll(y, shift_samples, axis=0)

def apply_gain_db(y, gain_db):
    g = 10.0 ** (gain_db / 20.0)
    return y * g

import os
import numpy as np
import random

def gen_augmentation(in_path, out_dir, max_gain_db=3.0, seed=None):
    """
    Read input audio at in_path, apply either:
      - circular modulo shift up to full length, or
      - gain change within ±max_gain_db
    Save a file to out_dir named as: original_filename + augmentation details.
    Returns the output file path and a description string.
    """

    os.makedirs(out_dir, exist_ok=True)

    # Original filename without extension
    base_name = os.path.splitext(os.path.basename(in_path))[0]

    # Read audio
    y = np.load(in_path)  # (samples, channels)
    if y.ndim == 1:
        return "Not enough dimensions"
    y = y.astype(np.float32)
    num_samples = y.shape[1]

    rng = random.Random()
    op = rng.choice(["shift", "gain"])

    if op == "shift":
        shift_samples = random.randint(0, max(0, num_samples - 1))
        y_aug = np.roll(y, shift_samples, axis=0)
        secs = shift_samples / SAMPLE_RATE
        filename = f"{base_name}_shift_{secs:.3f}s.npy"
        desc = f"modulo shift {shift_samples} samples ({secs:.3f}s)"
    else:
        gain_db = random.uniform(-max_gain_db, max_gain_db)
        gain = 10.0 ** (gain_db / 20.0)
        y_aug = y * gain
        filename = f"{base_name}_gain_{gain_db:+.2f}dB.npy"
        desc = f"gain {gain_db:+.2f} dB"

    # Clip and save
    y_aug = np.clip(y_aug, -1.0, 1.0)
    out_path = os.path.join(out_dir, filename)
    np.save(out_path, y_aug)

    print(f"Saved: {out_path} [{desc}]")
    return str(out_path), desc

visited_files = []

import os
import numpy as np
import soundfile as sf
import random
import string
from pathlib import Path

rng = random.Random()

for i in os.listdir(dst_dir):
    path = os.path.join(dst_dir, i)
    print(path)
    if path in visited_files:
        continue
    for j in range(rng.choice([1, 2, 3])):
        gen_augmentation(path, dst_dir)
    visited_files.append(path)

def _to_stereo_int16(x: np.ndarray) -> np.ndarray:
    """Convert float32 [-1,1] array [2,T] to int16 [T,2] for WAV output."""
    if x.ndim == 2 and x.shape[0] == 2:
        x = x.T
    x = np.clip(x, -1.0, 1.0)
    return (x * 32767.0).astype(np.int16)

import os
print(os.listdir("/content/data/"))
print(len(os.listdir("/content/data")))

import numpy as np

data = np.load('/content/data/13_5_18_24.wav.npy')

print(type(data))      # usually <class 'numpy.ndarray'>
print(data.shape)      # see the array dimensions
print(data.dtype)      # see the element type

print(data[0][:5])
print(data[1][:5])
print(max(data[0]))
print(max(data[1]))
print(min(data[0]))
print(min(data[1]))

del data
print("The 'data' variable has been deleted.")

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

def _norm(nc: int):
    # GroupNorm is stable for small batch sizes
    groups = max(1, nc // 8)
    return nn.GroupNorm(groups, nc)

_norm(120)

class ConvBlock1d(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1, k=5, p=None, d=1, act=True, dropout_rate=0.0):
        super().__init__()
        if p is None:
            p = (k - 1) // 2 * d
        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, stride=stride, padding=p, dilation=d)
        self.norm = _norm(out_ch)
        self.prelu = nn.PReLU(out_ch) if act else nn.Identity()
        self.dropout = nn.Dropout1d(dropout_rate) if dropout_rate > 0 else nn.Identity()

    def forward(self, x):
        x = self.conv(x)
        x = self.norm(x)
        x = self.prelu(x)
        x = self.dropout(x)
        return x

class UpConv1d(nn.Module):
    def __init__(self, in_ch, out_ch, k=4, s=2, p=1):
        super().__init__()
        self.tconv = nn.ConvTranspose1d(in_ch, out_ch, kernel_size=k, stride=s, padding=p)
        self.norm = _norm(out_ch)
        self.prelu = nn.PReLU(out_ch)

    def forward(self, x):
        x = self.tconv(x)
        x = self.norm(x)
        x = self.prelu(x)
        return x

class DilatedBottleneck(nn.Module):
    """Small context widener based on dilated convs."""
    def __init__(self, ch, num_layers, base_kernel=5, base_dilation=1):
        super().__init__()
        layers = []
        for i in range(num_layers):
            d = base_dilation * (2 ** i)
            layers += [ConvBlock1d(ch, ch, k=base_kernel, d=d)]
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

import os, math, random, glob
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from contextlib import nullcontext
from torch.cuda.amp import autocast, GradScaler
from tqdm.auto import tqdm
from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR

USE_CUDA = torch.cuda.is_available()
DEVICE = 'cuda' if USE_CUDA else 'cpu' # if on GPU

amp_ctx = torch.amp.autocast('cuda') if USE_CUDA else nullcontext()
scaler  = GradScaler(enabled=USE_CUDA)

import torch, os, subprocess, textwrap
print("torch.cuda.is_available():", torch.cuda.is_available())
print("torch.version.cuda:", torch.version.cuda)
print("CUDA devices:", torch.cuda.device_count())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

try:
    print(subprocess.check_output(["nvidia-smi"], text=True))
except Exception as e:
    print("nvidia-smi not available:", e)

USE_CUDA = torch.cuda.is_available()
assert USE_CUDA, "CUDA not available. Switch runtime to GPU."

print(USE_CUDA)
print(DEVICE)

from torch.utils.checkpoint import checkpoint

class AudioInpaintUNet(nn.Module):
    """
    U-Net encoder decoder for stereo audio inpainting.

    in_ch: 3 -> [L, R, mask]
    out_ch: 2 -> residual for [L, R]
    depth: number of downsampling stages
    base_ch: channels in first stage
    """
    def __init__(
        self,
        in_ch: int = 3,
        out_ch: int = 2,
        base_ch: int = BASE_CHANNELS,
        depth: int = U_NET_DEPTH,
        bottleneck_layers: int = BOTTLENECK_LAYERS,
        use_residual_composition: bool = True,
        dropout_rate: float = DROPOUT, # Added dropout rate parameter
        use_checkpointing: bool = True
    ):
        super().__init__()
        self.use_residual_composition = use_residual_composition
        self.depth = depth
        self.base_ch = base_ch
        self.chs = [base_ch * (2 ** i) for i in range(depth)]
        self.dropout_rate = dropout_rate # Store dropout rate
        self.use_checkpointing = use_checkpointing

        # Encoder
        enc = []
        prev = in_ch
        self.enc_downsamples = nn.ModuleList()
        self.enc_post = nn.ModuleList()
        for c in self.chs:
            # downsample by factor 2 with stride 2
            self.enc_downsamples.append(ConvBlock1d(prev, c, stride=2, k=5, dropout_rate=self.dropout_rate)) # Pass dropout_rate
            # refine at same resolution
            self.enc_post.append(ConvBlock1d(c, c, k=5, dropout_rate=self.dropout_rate)) # Pass dropout_rate
            prev = c

        # Bottleneck
        # Dropout inside DilatedBottleneck's ConvBlocks
        self.bottleneck = DilatedBottleneck(self.chs[-1], num_layers=bottleneck_layers) # DilatedBottleneck uses ConvBlock1d internally

        # Decoder
        self.up_layers = nn.ModuleList()
        self.dec_post = nn.ModuleList()
        # Iterate from the second to last encoder stage down to the first
        for i in range(depth - 1):
            # Upconv takes input from the previous decoder stage/bottleneck
            # Number of input channels to Upconv is channels of previous decoder stage
            # (initially bottleneck_channels = chs[-1])
            in_up_ch = self.chs[depth - 1 - i]
            # Upconv outputs channels to match the skip connection from encoder stage depth-2-i
            out_up_ch = self.chs[depth - 2 - i]
            # Dropout is not typically applied in UpConvTranspose layers directly,
            # but in the subsequent ConvBlock.
            self.up_layers.append(UpConv1d(in_up_ch, out_up_ch))

            # after concatenating skip: channels = out_up_ch + out_up_ch
            dec_in_ch = out_up_ch * 2
            dec_out_ch = out_up_ch
            self.dec_post.append(nn.Sequential(
                ConvBlock1d(dec_in_ch, dec_out_ch, k=5, dropout_rate=self.dropout_rate), # Pass dropout_rate
                ConvBlock1d(dec_out_ch, dec_out_ch, k=5, dropout_rate=self.dropout_rate), # Pass dropout_rate
            ))

        # Final projection to stereo residual
        # Input to final layer is output of last dec_post, which has chs[0] channels
        self.out = nn.Conv1d(self.chs[0], out_ch, kernel_size=3, padding=1)
        nn.init.zeros_(self.out.weight)
        nn.init.zeros_(self.out.bias)

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):
                nn.init.kaiming_normal_(m.weight, nonlinearity="leaky_relu")
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            # Dropout layers are handled by default PyTorch initialization

    def forward(self, x_masked_stereo: torch.Tensor, mask: torch.Tensor):
        """
        x_masked_stereo: [B, 2, T], values in [-1, 1], masked region is zeroed
        mask: [B, 1, T], binary {0,1}, 1 inside the hole
        returns: restored [B, 2, T], residual [B, 2, T]
        """
        assert x_masked_stereo.ndim == 3 and mask.ndim == 3
        assert x_masked_stereo.size(1) == 2 and mask.size(1) == 1
        B, _, T = x_masked_stereo.shape

        x = torch.cat([x_masked_stereo, mask], dim=1)  # [B, 3, T]

        skips = []
        # Encoder
        for down, post in zip(self.enc_downsamples, self.enc_post):
            if self.use_checkpointing and self.training:
                def _enc_step(inp):
                    y = down(inp)
                    y = post(y)
                    return y
                x = checkpoint(_enc_step, x)
            else:
                x = down(x)
                x = post(x)
            skips.append(x)

        # Bottleneck
        if self.use_checkpointing and self.training:
            x = checkpoint(self.bottleneck, x)
        else:
            x = self.bottleneck(x) # Dropout applied inside DilatedBottleneck's ConvBlocks

        # Decoder
        for i in range(self.depth - 1):
            skip = skips[self.depth - 2 - i]

            def _dec_step(inp, sk):
                y = self.up_layers[i](inp)
                if y.size(-1) != sk.size(-1):
                    diff = sk.size(-1) - y.size(-1)
                    y = F.pad(y, (0, diff)) if diff > 0 else y[..., :sk.size(-1)]
                y = torch.cat([y, sk], dim=1)
                y = self.dec_post[i](y)
                return y

            if self.use_checkpointing and self.training:
                x = checkpoint(_dec_step, x, skip)
            else:
                x = _dec_step(x, skip)


        residual = self.out(x)  # [B, 2, T]
        # adjust to original T
        if residual.size(-1) != T:
            diff = T - residual.size(-1)
            residual = F.pad(residual, (0, diff)) if diff > 0 else residual[..., :T]

        if self.use_residual_composition:
            m2 = mask.repeat(1, 2, 1)  # broadcast to stereo
            restored = x_masked_stereo + residual * m2
        else:
            restored = residual  # treat as direct output

        return restored, residual

def masked_l1(restored, gt, mask):
    """
    restored, gt: [B, 2, T]
    mask:        [B, 1, T]  with 1s inside the hole, 0 elsewhere
    """
    m2 = mask.repeat(1, 2, 1)
    err = (restored - gt).abs() * m2
    return err.sum() / (m2.sum() + 1e-8)

def edge_ring(mask, width=256):
    """
    mask:  [B, 1, T] (binary 0/1)
    Returns a boundary ring mask [B, 1, T] of 0/1 near transitions of mask.
    """
    B, _, T = mask.shape
    # edge indicator: 1 where mask changes between adjacent samples
    diff = F.pad(mask[:, :, 1:] != mask[:, :, :-1], (1, 0), value=False).float()  # [B,1,T]
    # spread edges by 'width' using conv with ones
    k = 2*width + 1
    w = torch.ones(1, 1, k, device=mask.device)
    ring = F.conv1d(diff, w, padding=width)
    ring = (ring > 0).float()
    return ring

def boundary_l1(restored, gt, mask, width=256):
    ring = edge_ring(mask, width=width)
    r2 = ring.repeat(1, 2, 1)
    err = (restored - gt).abs() * r2
    return err.sum() / (r2.sum() + 1e-8)

def si_sdr_loss(restored, gt, weight=None, eps=1e-8):
    """
    restored, gt: [B, 2, T]
    weight:       [B, 1, T] or None (if None, uses uniform weighting)
    Returns: negative SI-SDR (mean over batch & channels)
    """
    if weight is None:
        weight = torch.ones_like(gt[:, :1, :])

    # expand weight to stereo
    w = weight.repeat(1, 2, 1)
    x = restored
    s = gt

    # weighted mean removal (scale-invariant works best with zero-mean)
    def wmean(z, w):
        return (z * w).sum(-1, keepdim=True) / (w.sum(-1, keepdim=True) + eps)

    x = x - wmean(x, w)
    s = s - wmean(s, w)

    # weighted dot products
    def wdot(a, b, w):
        return (a * b * w).sum(-1, keepdim=True)

    s_target_scale = wdot(x, s, w) / (wdot(s, s, w) + eps)
    s_target = s_target_scale * s
    e_noise = x - s_target

    num = (s_target**2 * w).sum(-1)
    den = (e_noise**2 * w).sum(-1)
    si_sdr = 10.0 * torch.log10((num + eps) / (den + eps))  # [B, 2]

    # loss is negative SI-SDR (maximize SI-SDR)
    return -(si_sdr.mean())

BOUNDARY_WIDTH = 512
W_L1_HOLE   = 1.0 # previously 0.0
W_L1_TRANSITION  = 0.1 # previously 0.25
W_SI_SDR   = 0.0 # previously 1.0

def total_loss(restored, gt, mask, ramp_steps=3000):
    # Optional warmup for SI-SDR (stabilizes early training)

    l_gap  = masked_l1(restored, gt, mask)
    l_seam = boundary_l1(restored, gt, mask, width=BOUNDARY_WIDTH)
    # Focus SI-SDR on the hole; include rim if you like:
    # rim = edge_ring(mask, width=BOUNDARY_WIDTH)
    # l_sdr = si_sdr_loss(restored, gt, weight=torch.clamp(mask + 0.25*rim, 0, 1))
    l_sdr  = si_sdr_loss(restored, gt, weight=mask)

    loss = W_L1_HOLE*l_gap + W_L1_TRANSITION*l_seam + W_SI_SDR*l_sdr
    logs = {"l_gap": float(l_gap.detach().cpu()),
            "l_seam": float(l_seam.detach().cpu()),
            "l_sdr": float(l_sdr.detach().cpu())}
    return loss, logs

@torch.no_grad()
def sanity_checks(gt, loss_fn):
    # shapes: [B, C, T]

    B, C, T = gt.shape

    start = T // 4
    end = 3 * T // 4
    size = end-start
    mask = torch.zeros((1, 1, T), device=gt.device)
    mask[:, :, start:end] = 1

    zeroes = zeroes = torch.zeros_like(gt)

    half_gt = gt/2

    noise = torch.empty_like(gt).uniform_(gt.min(), gt.max())

    variance_scale = 0.001  # smaller = milder
    moderate_noise = gt + torch.randn_like(gt) * variance_scale

    print("", start, end, start, "+", start + size // 4, start + size * 3 // 4, "+", end)

    low_quality_seam = gt.clone()

    # Fill first seam region with random values in [min, max]
    low_quality_seam[:, :, start:start+size//4] = torch.rand_like(
        low_quality_seam[:, :, start:start+size//4]
    ) * (gt.max() - gt.min()) + gt.min()

    # Fill second seam region with either a constant or random values
    low_quality_seam[:, :, start+size*3//4:end] = torch.rand_like(
        low_quality_seam[:, :, start+size*3//4:end]
    ) * (gt.max() - gt.min()) + gt.min()

    high_quality_seam = gt.clone()

    # Fill first seam region with random values in [min, max]
    high_quality_seam[:, :, start+size//4:start+size*3//4] = torch.rand_like(
        high_quality_seam[:, :, start+size//4:start+size*3//4]
    ) * (gt.max() - gt.min()) + gt.min()

    cases = {
        "perfect_gt": gt,
        "zeroes": zeroes,
        "half_gt": half_gt,
        "noise": noise,
        "moderate_noise": moderate_noise,
        "low_quality_seam": low_quality_seam,
        "high_quality_seam": high_quality_seam,
    }

    results = {}
    for name, y in cases.items():
        total, logs = loss_fn(y, gt, mask)
        results[name] = {"total": float(total), **{k: float(v) for k,v in logs.items()}}
    return results

test_gt = torch.from_numpy(np.load("/content/data/10_7_24_30.wav.npy")[:, 0:SAMPLE_RATE]).unsqueeze(0)
print("Input shape", test_gt.shape)
sanity_checks(test_gt, total_loss)

test_gt.dtype

total_loss

import os
import glob
import numpy as np
import json
from tqdm.auto import tqdm

# Path to your audio files
audio_dir = INPUT_DIR
all_audio_files = glob.glob(os.path.join(audio_dir, '*.npy'))

# Define the path for the lengths file
lengths_file = os.path.join(INPUT_DIR, 'audio_lengths.json')

# Check if the lengths file already exists
if os.path.exists(lengths_file):
    print(f"Lengths file already exists at {lengths_file}. Loading lengths.")
    with open(lengths_file, 'r') as f:
        audio_lengths = json.load(f)
else:
    print(f"Lengths file not found at {lengths_file}. Computing and saving lengths.")
    audio_lengths = {}
    # Iterate through all audio files and get their lengths
    for f in tqdm(all_audio_files, desc="Computing Audio Lengths"):
        try:
            # Use mmap_mode='r' to read shape without loading full data into memory
            length = np.load(f, mmap_mode='r').shape[-1]
            audio_lengths[f] = length
        except Exception as e:
            print(f"Error processing file {f}: {e}. Skipping.")

    # Save the computed lengths to a JSON file
    with open(lengths_file, 'w') as f:
        json.dump(audio_lengths, f)

    print(f"Audio lengths computed and saved to {lengths_file}")

# Now 'audio_lengths' dictionary contains {file_path: length} for all processed files
print(f"Loaded/Computed lengths for {len(audio_lengths)} audio files.")

import os
import glob
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split # Use sklearn for splitting

# Path to your audio files
audio_dir = INPUT_DIR
all_audio_files = glob.glob(os.path.join(audio_dir, '*.npy'))

# 1. Identify Unique Sources
# Extract the source ID (part before the first underscore) from each filename
source_ids = [os.path.basename(f).split('_')[0] for f in all_audio_files]
unique_source_ids = list(set(source_ids))

print(f"Found {len(all_audio_files)} total audio files.")
print(f"Found {len(unique_source_ids)} unique audio sources.")

# 2. Split Sources into Training and Validation Sets
# Use stratify if you have multiple files per source and want balanced splits
# For simplicity, random split of unique sources:
train_sources, val_sources = train_test_split(unique_source_ids, test_size=0.2, random_state=42) # Add random_state for reproducibility

print(f"Training sources: {len(train_sources)}")
print(f"Validation sources: {len(val_sources)}")

# Create lists of files belonging to the train and validation sources
train_files = [f for f in all_audio_files if os.path.basename(f).split('_')[0] in train_sources]
val_files = [f for f in all_audio_files if os.path.basename(f).split('_')[0] in val_sources]

print(f"Training files: {len(train_files)}")
print(f"Validation files: {len(val_files)}")

print([f for f in train_files if audio_lengths.get(f, 0) >= WIN_LEN])

import os
import numpy as np
import torch
from torch.utils.data import Dataset
import random
import json
from torch.utils.data import get_worker_info

class AudioInpaintingDataset(Dataset):
    def __init__(self, audio_files, win_len=WIN_LEN, audio_lengths=None):
        self.win_len = win_len
        # Filter files shorter than win_len immediately during initialization
        # Use the pre-computed audio_lengths dictionary if provided
        self.audio_files = [f for f in audio_files if audio_lengths.get(f, 0) >= self.win_len]
        print(f"Initialized dataset with {len(self.audio_files)} files meeting minimum length using pre-computed lengths.")


    def __len__(self):
        return len(self.audio_files)

    def __getitem__(self, idx):
        info = get_worker_info()
        if info is not None and idx == 0:  # print once per worker at start
            print(f"DataLoader worker {info.id} of {info.num_workers}")
        # Load an audio file by index
        audio_path = self.audio_files[idx]
        try:
            # Use mmap_mode='r' to avoid loading the whole file into memory at once if possible,
            # though numpy slicing will still load the segment.
            # Load the full file, but only for the selected segment, and copy it.
            # Loading the segment with mmap='r' and then copying ensures we only load
            # the necessary part into a writable numpy array before converting to torch.
            audio_data_mmap = np.load(audio_path, mmap_mode='r')

            # --- Data Preparation ---
            T = audio_data_mmap.shape[-1] # Total length of the audio

            # Select a random start position for the segment
            # Ensure there's enough space for the window (win_len)
            max_start_idx = T - self.win_len
            start_idx = np.random.randint(0, max_start_idx + 1)
            segment = audio_data_mmap[:, start_idx:start_idx + self.win_len].copy() # [2, win_len] - .copy() makes it writable
            gt = torch.from_numpy(segment).float() # Ground truth - now from a writable array

            # Create a mask with a random hole within the segment
            mask = torch.zeros(1, self.win_len) # [1, win_len]

            # Determine random start and end for the hole within the segment
            # Ensure hole_end is at least hole_start + 1 and within win_len
            hole_length = np.random.randint(MIN_HOLE_LENGTH, MAX_HOLE_LENGTH) # Keep hole length consistent

            # Calculate the bounds for the hole start to avoid first/last 20%
            safe_zone = int(self.win_len * 0.2)
            # Hole start must be after the first safe_zone samples
            # Hole end (hole_start + hole_length) must be before the last safe_zone samples
            min_hole_start = safe_zone
            max_hole_start = self.win_len - safe_zone - hole_length

            # Ensure max_hole_start is not less than min_hole_start (in case safe_zone is too large)
            if max_hole_start < min_hole_start:
                # If the window is too small to have a hole outside the safe zones,
                # throw an error
                raise Exception("The audio needs to be large enough for a hole to be able to be included without overlapping with the safe zones")
            else:
                hole_start_in_segment = np.random.randint(min_hole_start, max_hole_start + 1)


            mask[:, hole_start_in_segment:hole_start_in_segment + hole_length] = 1.0

            # Create the masked input with scaling instead of zeros
            m2 = mask.repeat(2, 1)  # [2, T]

            hole_end = hole_start_in_segment + hole_length

            # Start from clean audio, then modify only the masked region
            rng = random.Random()
            x_masked_stereo = gt.clone()  # [2, win_len]
            x_masked_stereo[:, hole_start_in_segment:hole_end] = rng.uniform(0.5, 2.0) * gt[:, hole_start_in_segment:hole_end]

            # Optional: keep inputs in range if your pipeline expects [-1, 1]
            x_masked_stereo = x_masked_stereo.clamp(-1.0, 1.0)

            return x_masked_stereo, mask, gt

        except Exception as e:
            print(f"Error loading or processing file {audio_path}: {e}")
            # Re-raise the exception to be caught by the DataLoader worker.
            # For robust training with potential bad files, a custom collate_fn returning None
            # and filtering None batches is better, but more complex to modify in place.
            raise e

import os
import glob
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split # Use sklearn for splitting

# Path to your audio files
audio_dir = INPUT_DIR
all_audio_files = glob.glob(os.path.join(audio_dir, '*.npy'))

# 1. Identify Unique Sources
# Extract the source ID (part before the first underscore) from each filename
source_ids = [os.path.basename(f).split('_')[0] for f in all_audio_files]
unique_source_ids = list(set(source_ids))

print(f"Found {len(all_audio_files)} total audio files.")
print(f"Found {len(unique_source_ids)} unique audio sources.")

# 2. Split Sources into Training and Validation Sets
# Use stratify if you have multiple files per source and want balanced splits
# For simplicity, random split of unique sources:
train_sources, val_sources = train_test_split(unique_source_ids, test_size=0.2, random_state=42) # Add random_state for reproducibility

print(f"Training sources: {len(train_sources)}")
print(f"Validation sources: {len(val_sources)}")

# Create lists of files belonging to the train and validation sources
train_files = [f for f in all_audio_files if os.path.basename(f).split('_')[0] in train_sources]
val_files = [f for f in all_audio_files if os.path.basename(f).split('_')[0] in val_sources]

print(f"Training files: {len(train_files)}")
print(f"Validation files: {len(val_files)}")
print(audio_lengths)


# 3. Create Dataset Instances using the filtered file lists and pre-computed lengths
# The dataset __init__ will now filter by win_len using the provided audio_lengths
train_dataset = AudioInpaintingDataset(train_files, win_len=WIN_LEN, audio_lengths=audio_lengths)
val_dataset = AudioInpaintingDataset(val_files, win_len=WIN_LEN, audio_lengths=audio_lengths)


# 4. Create data loaders
# Consider setting num_workers to 0 initially if you faced OSErrors with Drive
train_loader = DataLoader(
    train_dataset, batch_size=BATCH_SZ, shuffle=True,
    num_workers=NUM_WORKERS, persistent_workers=False, # persistent_workers=True when NUM_WORKERS = 2
    pin_memory=True, prefetch_factor=None # prefetch_factor=4 when NUM_WORKERS = 2
)
val_loader = DataLoader(
    val_dataset, batch_size=BATCH_SZ, shuffle=False,
    num_workers=NUM_WORKERS, persistent_workers=False, # persistent_workers=True when NUM_WORKERS = 2
    pin_memory=True, prefetch_factor=None # prefetch_factor=4 when NUM_WORKERS = 2
)


print(f"\nTraining set size (after win_len filter): {len(train_dataset)}")
print(f"Validation set size (after win_len filter): {len(val_dataset)}")
print(f"Training DataLoader created with batch size {BATCH_SZ} and num_workers={train_loader.num_workers}")
print(f"Validation DataLoader created with batch size {BATCH_SZ} and num_workers={val_loader.num_workers}")

import random

# Get a random index from the training dataset
random_index = random.randint(0, len(train_dataset) - 1)

# Get the sample using the random index
x_masked_stereo_sample, mask_sample, gt_sample = train_dataset[random_index]

print(f"Shape of masked stereo audio sample: {x_masked_stereo_sample.shape}")
print(f"Shape of mask sample: {mask_sample.shape}")
print(f"Shape of ground truth sample: {gt_sample.shape}")

print("Train size:", len(train_dataset), "Val size:", len(val_dataset))
assert len(train_dataset) > 0 and len(val_dataset) > 0

"""Now, let's add the code to profile the model's performance. We will run a few steps of the training loop within the profiler's context."""

import math, torch

print("Sanity check…")
print("Depth:", U_NET_DEPTH, "Window length:", WIN_LEN, "Bottleneck size", BOTTLENECK_LAYERS)

model = torch.compile(AudioInpaintLSTM(
    base_ch=64,
    ds_factors=(4, 4),     # total stride 16
    lstm_hidden=256,
    lstm_layers=3,
    bidirectional=True,
    dropout=0.2,
    use_residual_composition=True,
)).to(DEVICE)
model.train()

# Grab one batch
xb, mb, yb = next(iter(train_loader))
print("Batch shapes (CPU):", xb.shape, mb.shape, yb.shape)

xb = xb.to(DEVICE, non_blocking=True)
mb = mb.to(DEVICE, non_blocking=True)
yb = yb.to(DEVICE, non_blocking=True)

print("Batch shapes (GPU):", xb.shape, mb.shape, yb.shape)

# Forward once
with amp_ctx:
    out, res = model(xb, mb)
    loss, logs = total_loss(out, yb, mb)

print("Forward OK. out:", out.shape, "res:", res.shape, "loss:", loss.item())

# ONLY RUN IF CLEARING THE MODEL

import gc
import torch

def clear_memory():
    # Clear CUDA cache
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("CUDA cache cleared.")

    global model, optimizer, scaler

    # Delete model, optimizer, and scaler variables if they exist
    if 'model' in globals():
        del model
        print("'model' deleted.")
    if 'optimizer' in globals():
        del optimizer
        print("'optimizer' deleted.")
    if 'scaler' in globals():
        del scaler
        print("'scaler' deleted.")

    torch.cuda.empty_cache()

    # Run garbage collection
    gc.collect()
    print("Garbage collection complete.")

print(torch.cuda.memory_summary(device=None, abbreviated=False))

clear_memory()

import os, glob, itertools, torch
from tqdm import tqdm
import torch.profiler as profiler
from torch.cuda.amp import GradScaler

model = torch.compile(AudioInpaintUNet()).to(DEVICE)
optimizer = torch.optim.AdamW(model.parameters(), lr=start_LR, weight_decay=WD)
scaler = GradScaler(enabled=USE_CUDA)

total_logical_steps = MAX_STEPS
warmup_steps = max(1, int(0.05 * total_logical_steps))

warmup = LinearLR(
    optimizer,
    start_factor=1e-3,   # start at 0.001x of start_LR
    end_factor=1.0,
    total_iters=warmup_steps
)

cosine = CosineAnnealingLR(
    optimizer,
    T_max=total_logical_steps - warmup_steps,
    eta_min=end_LR
)

scheduler = SequentialLR(
    optimizer,
    schedulers=[warmup, cosine],
    milestones=[warmup_steps]
)

num_logical_steps = 10
profile_dir = "/content/profiling_results"
os.makedirs(profile_dir, exist_ok=True)
initial_files = set(glob.glob(os.path.join(profile_dir, "*")))

num_microsteps = num_logical_steps * accumulation_steps  # bars you expect

prof = profiler.profile(
    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],
    schedule=profiler.schedule(wait=0, warmup=1, active=num_microsteps, repeat=1),
    on_trace_ready=profiler.tensorboard_trace_handler(profile_dir),
    record_shapes=False,
    with_stack=False,
)

data_iter = itertools.cycle(train_loader)
model.train()
optimizer.zero_grad(set_to_none=True)

with prof:
    for t in tqdm(range(num_logical_steps), desc="Logical steps"):
        for k in range(accumulation_steps):
            batch = next(data_iter)
            if batch is None:
                print("BATCH DOES NOT EXIST")
                continue
            x_masked_stereo, mask, gt = batch
            x_masked_stereo = x_masked_stereo.to(DEVICE, non_blocking=True)
            mask = mask.to(DEVICE, non_blocking=True)
            gt = gt.to(DEVICE, non_blocking=True)

            with amp_ctx:
                restored, residual = model(x_masked_stereo, mask)
                loss, logs = total_loss(restored, gt, mask)

            scaler.scale(loss / accumulation_steps).backward()

            # one bar per microstep
            torch.cuda.synchronize()       # optional but makes timings cleaner
            prof.step()

        scaler.step(optimizer)             # once per logical step
        scaler.update()
        scheduler.step()
        optimizer.zero_grad(set_to_none=True)

print(f"Profiling finished. Results saved to {profile_dir}")

final_files = set(glob.glob(os.path.join(profile_dir, "*")))
new_files = list(final_files - initial_files)
print("\nGenerated profiling trace files:")
print("\n".join(f"- {os.path.basename(f)}" for f in new_files) or "No new trace files were generated during this run.")
print("\nYou can analyze the results using TensorBoard.")

print("cec8b2ad8104_2388.1761025230797104844.pt.trace.json" in os.listdir("/content/profiling_results"))
print(os.path.join("/content/profiling_results", "cec8b2ad8104_2388.1761025230797104844.pt.trace.json"))

"""After running the profiling code, you can use TensorBoard to visualize the results. Run the following commands in a new code cell to start TensorBoard. Click on the generated link to open it."""

!pip install -U torch-tb-profiler

os.makedirs("/content/drive/MyDrive/Profiling_Results/V10", exist_ok=True)
!cp -r "/content/profiling_results/cec8b2ad8104_2388.1761025230797104844.pt.trace.json" "/content/drive/MyDrive/Profiling_Results/V10"

!ls -R "/content/drive/MyDrive/Profiling_Results/V10"

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir "/content/drive/MyDrive/Profiling_Results/V6_4"

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir "/content/drive/MyDrive/Profiling_Results/V10"

"""Inside TensorBoard, navigate to the "Profile" tab to see the detailed profiling report. Look for operations that consume the most time on the GPU and CPU. This will help identify bottlenecks."""

print(train_files[0:10])
print("_____________________________________________")
print(val_files[0:10])

print(len(train_loader))

print(BATCH_SZ)

train_losses = []
val_losses = []

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb:128"
# Optional, avoids heavy autotune workspaces that can spike memory
os.environ["TORCHINDUCTOR_MAX_AUTOTUNE_GEMM"] = "0"

from tqdm import tqdm

# Initialize model, optimizer, and scaler
import itertools, time, os, torch
from torch.cuda.amp import GradScaler

model = torch.compile(AudioInpaintUNet()).to(DEVICE)
optimizer = torch.optim.AdamW(model.parameters(), lr=start_LR, weight_decay=WD)
scaler = GradScaler(enabled=USE_CUDA)

total_logical_steps = MAX_STEPS
warmup_steps = max(1, int(0.05 * total_logical_steps))

warmup = LinearLR(
    optimizer,
    start_factor=1e-3,   # start at 0.001x of start_LR
    end_factor=1.0,
    total_iters=warmup_steps
)

cosine = CosineAnnealingLR(
    optimizer,
    T_max=total_logical_steps - warmup_steps,
    eta_min=end_LR
)

scheduler = SequentialLR(
    optimizer,
    schedulers=[warmup, cosine],
    milestones=[warmup_steps]
)

print("Model, optimizer, and scaler initialized.")

# Training control
step = 0  # logical step counter
best_val_loss = float('inf')
print(f"Starting training on {DEVICE}")

# Timing
training_start_time_abs = time.time()
print(f"Training started at absolute time: {training_start_time_abs:.4f} seconds")

# Cycling data iterator like the profiling loop
data_iter = itertools.cycle(train_loader)

# Accumulators over the current validation window
total_train_loss = 0.0
train_logs = {"l_gap": 0.0, "l_seam": 0.0, "l_sdr": 0.0}
num_train_batches = 0

model.train()
optimizer.zero_grad(set_to_none=True)

# Create directory for loss logs
log_dir = "/content/errors"
os.makedirs(log_dir, exist_ok=True)
loss_log_path = os.path.join(log_dir, "loss.log")

num_val_loops = 5 # Define the number of times to loop through the validation loader

try:
    with tqdm(total=MAX_STEPS, initial=step, desc="Train steps", dynamic_ncols=True) as pbar:
        while step < MAX_STEPS:
            # One logical step equals accumulation_steps microsteps
            accumulation_start_time = time.time() - training_start_time_abs

            for k in range(accumulation_steps):
                x_masked_stereo, mask, gt = next(data_iter)
                x_masked_stereo = x_masked_stereo.to(DEVICE, non_blocking=True)
                mask = mask.to(DEVICE, non_blocking=True)
                gt = gt.to(DEVICE, non_blocking=True)

                with amp_ctx:
                    restored, residual = model(x_masked_stereo, mask)
                    loss, logs = total_loss(restored, gt, mask) # Removed step and ramp_steps here

                scaler.scale(loss / accumulation_steps).backward()

                total_train_loss += loss.item()
                train_logs["l_gap"] += logs["l_gap"]
                train_logs["l_seam"] += logs["l_seam"]
                train_logs["l_sdr"] += logs["l_sdr"]
                num_train_batches += 1

                if USE_CUDA:
                    torch.cuda.synchronize()

            # Optimizer step once per logical step
            accumulation_end_time = time.time() - training_start_time_abs
            accumulation_duration = accumulation_end_time - accumulation_start_time

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)
            scheduler.step()

            step += 1
            pbar.update(1)  # progress bar tick

            avg_train_loss = total_train_loss / num_train_batches
            avg_train_logs = {k: v / num_train_batches for k, v in train_logs.items()}
            train_losses.append({
                "step": step,
                "total_loss": avg_train_loss,
                "l_gap": avg_train_logs["l_gap"],
                "l_seam": avg_train_logs["l_seam"],
                "l_sdr": avg_train_logs["l_sdr"]
            })


            if step >= MAX_STEPS:
                break

            # Validation every VAL_EVERY logical steps
            if step % VAL_EVERY == 0:
                print("\n" + "=" * 50)
                print(f"=== Starting Validation at Step {step} ===")
                val_start_time = time.time() - training_start_time_abs

                model.eval()
                total_val_loss = 0.0
                val_logs = {"l_gap": 0.0, "l_seam": 0.0, "l_sdr": 0.0}
                num_val_batches = 0

                for val_loop_idx in range(num_val_loops): # Loop multiple times
                    print(f"  Validation Pass {val_loop_idx + 1}/{num_val_loops}")
                    for x_masked_stereo_val, mask_val, gt_val in tqdm(
                        val_loader, desc=f"Step {step}/{MAX_STEPS} (Val Pass {val_loop_idx + 1})", leave=False, dynamic_ncols=True
                    ):
                        x_masked_stereo_val = x_masked_stereo_val.to(DEVICE, non_blocking=True)
                        mask_val = mask_val.to(DEVICE, non_blocking=True)
                        gt_val = gt_val.to(DEVICE, non_blocking=True)

                        with amp_ctx:
                            restored_val, residual_val = model(x_masked_stereo_val, mask_val)
                            val_loss, logs_val = total_loss(restored_val, gt_val, mask_val)

                        total_val_loss += val_loss.item()
                        val_logs["l_gap"] += logs_val["l_gap"]
                        val_logs["l_seam"] += logs_val["l_seam"]
                        val_logs["l_sdr"] += logs_val["l_sdr"]
                        num_val_batches += 1

                val_end_time = time.time() - training_start_time_abs
                val_duration = val_end_time - val_start_time
                print(f"=== Finished Validation at Step {step} ===")
                print(
                    f"Validation | Start: {val_start_time:.4f}s | End: {val_end_time:.4f}s | "
                    f"Duration: {val_duration:.4f} seconds."
                )
                print("=" * 50 + "\n")
                print(f"step {step} lr {scheduler.get_last_lr()[0]:.6e}")
                print("=" * 50 + "\n")
                print("\n")

                # Report averages and update tqdm postfix
                avg_val_loss = total_val_loss / max(1, num_val_batches)
                avg_val_logs = {k: v / max(1, num_val_batches) for k, v in val_logs.items()}


                print(
                    f"Step {step}: Val Loss = {avg_val_loss:.4f}"
                )
                print(
                    "  Val Logs:   "
                    f"l_gap={avg_val_logs['l_gap']:.4f}, "
                    f"l_seam={avg_val_logs['l_seam']:.4f}, "
                    f"l_sdr={avg_val_logs['l_sdr']:.4f}"
                )

                # Append validation losses
                val_losses.append({
                    "step": step,
                    "total_loss": avg_val_loss,
                    "l_gap": avg_val_logs["l_gap"],
                    "l_seam": avg_val_logs["l_seam"],
                    "l_sdr": avg_val_logs["l_sdr"]
                })

                # Log the loss for this microstep
                with open(loss_log_path, "a") as f:
                    f.write(f"Step: {step}\n")
                    f.write(f"l_gap: {avg_val_logs['l_gap']}\n")
                    f.write(f"l_seam: {avg_val_logs['l_seam']}\n")
                    f.write(f"l_sdr: {avg_val_logs['l_sdr']}\n")
                    f.write("_"*50)


                # Checkpoint best
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    checkpoint_path = os.path.join(CKPT_DIR, f"best_model_step_{step}.pth")
                    torch.save(
                        {
                            "step": step,
                            "model_state_dict": model.state_dict(),
                            "optimizer_state_dict": optimizer.state_dict(),
                            "best_val_loss": best_val_loss,
                        },
                        checkpoint_path,
                    )
                    print(f"Saved best model checkpoint to {checkpoint_path}")


                model.train()

except Exception as e:
    print(f"An error occurred during training: {e}")

print("Training finished.")

print(train_loader)

import os
import torch
import numpy as np
from tqdm import tqdm
from scipy.io import wavfile

SAVE_DIR = "/content/eval"
os.makedirs(SAVE_DIR, exist_ok=True)

@torch.no_grad()
def evaluate_and_save_audio(model_class, checkpoint_path, loader,
                            total_loss_fn, device='cuda',
                            sample_rate=48000, limit=None):
    print(f"Loading model from: {checkpoint_path}")
    model = torch.compile(AudioInpaintUNet()).to(device)
    state = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(state["model_state_dict"])  # keys will match
    model.eval()


    total_loss = 0.0
    total_logs = {"l_gap": 0.0, "l_seam": 0.0, "l_sdr": 0.0}
    num_batches = 0
    saved = 0
    idx = 0

    for batch in tqdm(loader, desc="Evaluating"):
        x_masked_stereo, mask, gt = [t.to(device) for t in batch]
        restored, _ = model(x_masked_stereo, mask)
        loss, logs = total_loss_fn(restored, gt, mask)

        print("\n============================")
        print(loss)
        print("============================")

        total_loss += loss.item()
        for k in total_logs:
            total_logs[k] += logs[k]
        num_batches += 1

        # save audio
        x_masked_np = x_masked_stereo.cpu().numpy()
        restored_np = restored.cpu().numpy()
        gt_np = gt.cpu().numpy()

        B = x_masked_np.shape[0]
        for b in range(B):
            base = f"val_{idx:05d}"
            wavfile.write(os.path.join(SAVE_DIR, f"{base}_masked.wav"),
                          sample_rate, _to_stereo_int16(x_masked_np[b]))
            wavfile.write(os.path.join(SAVE_DIR, f"{base}_restored.wav"),
                          sample_rate, _to_stereo_int16(restored_np[b]))
            wavfile.write(os.path.join(SAVE_DIR, f"{base}_gt.wav"),
                          sample_rate, _to_stereo_int16(gt_np[b]))
            idx += 1
            saved += 3
        if limit is not None and idx >= limit:
            break

    avg_loss = total_loss / max(1, num_batches)
    avg_logs = {k: v / max(1, num_batches) for k, v in total_logs.items()}

    print(f"\n=== Evaluation Complete ===")
    print(f"Validation Loss: {avg_loss:.4f}")
    for k, v in avg_logs.items():
        print(f"{k}: {v:.4f}")
    print(f"Saved {saved} WAV files in {SAVE_DIR}")
    print("============================")

checkpoint_path = "/content/drive/MyDrive/Checkpoints/best_model_step_1500.pth"
print(os.path.exists(checkpoint_path))
evaluate_and_save_audio(AudioInpaintUNet, checkpoint_path, val_loader,
                        total_loss, device=DEVICE, sample_rate=SAMPLE_RATE, limit=5)

import matplotlib.pyplot as plt

def plot_loss(x, train_y, val_y, loss_name):
    plt.figure()
    plt.plot(x, train_y, label='Training', marker='o')
    plt.plot(x, val_y, label='Validation', marker='o')
    plt.xlabel('Step')
    plt.ylabel(loss_name)
    plt.title(f'{loss_name} over Steps')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

steps = [x*250 for x in range(1, 14)]
train_total_loss = [step["total_loss"] for step in train_losses if step["step"]%250 == 0 and step["step"] != 20000]
val_total_loss = [step["total_loss"] for step in val_losses if step["step"]%250 == 0]

plot_loss(steps, train_total_loss, val_total_loss, "Total Loss")

val_losses

def remap_state_dict_keys(sd):
    out = {}
    for k, v in sd.items():
        if k.startswith("_orig_mod."):
            k = k[len("_orig_mod."):]
        if k.startswith("module."):
            k = k[len("module."):]
        out[k] = v
    return out

import torch, numpy as np, os
from tqdm import tqdm
import matplotlib.pyplot as plt

@torch.no_grad()
def eval_inhole_metrics(model, val_loader, device='cuda', sample_rate=48000,
                        save_diff_png=None, limit=8):
    model.eval()
    totals = {
        "mae_in": 0.0, "mae_out": 0.0,
        "sdr_in_model": 0.0, "sdr_in_baseline": 0.0,
        "count": 0
    }

    def mae(a, b, w):
        # a, b [B, 2, T], w [B, 1, T] in {0,1}
        m = w.repeat(1, 2, 1)
        num = torch.sum(torch.abs(a - b) * m)
        den = torch.sum(m) + 1e-8
        return (num / den).item()

    def si_sdr(x, s, w, eps=1e-8):
        # scale invariant SDR on weighted region
        w2 = w.repeat(1, 2, 1)
        def wmean(z): return torch.sum(z * w2, dim=-1, keepdim=True) / (torch.sum(w2, dim=-1, keepdim=True) + eps)
        x0 = x - wmean(x)
        s0 = s - wmean(s)
        num = torch.sum(x0 * s0 * w2, dim=-1, keepdim=True)
        den = torch.sum(s0 * s0 * w2, dim=-1, keepdim=True) + eps
        s_target = num / den * s0
        e = x0 - s_target
        sdr = 10.0 * torch.log10((torch.sum(s_target**2 * w2, dim=-1) + eps) / (torch.sum(e**2 * w2, dim=-1) + eps))
        return torch.mean(sdr).item()

    saved = 0
    for bidx, (x_masked, mask, gt) in enumerate(tqdm(val_loader, desc="Eval in-hole")):
        x_masked = x_masked.to(device)
        mask = mask.to(device)
        gt = gt.to(device)

        restored, _ = model(x_masked, mask)

        # in-hole and out-of-hole masks
        w_in = mask
        w_out = 1.0 - mask

        # MAE
        mae_in = mae(restored, gt, w_in)
        mae_out = mae(restored, gt, w_out)

        # SDR in-hole for model vs baseline
        sdr_in_model = si_sdr(restored, gt, w_in)
        sdr_in_baseline = si_sdr(x_masked, gt, w_in)

        totals["mae_in"] += mae_in
        totals["mae_out"] += mae_out
        totals["sdr_in_model"] += sdr_in_model
        totals["sdr_in_baseline"] += sdr_in_baseline
        totals["count"] += 1

        # optional diff spectrogram of first item in batch
        if save_diff_png and saved < limit:
            r = restored[0].detach().float().cpu().numpy()    # [2, T]
            xm = x_masked[0].detach().float().cpu().numpy()
            m = mask[0].detach().float().cpu().numpy()        # [1, T]
            g = gt[0].detach().float().cpu().numpy()

            diff = r - xm  # model change
            # simple magnitude "spectrogram" via FFT for quick visualization
            mag = np.abs(np.fft.rfft(diff[0]))  # left channel
            plt.figure(figsize=(6,3))
            plt.plot(20*np.log10(np.maximum(mag, 1e-8)))
            plt.title("Model change vs masked input (dB) left channel")
            out_path = os.path.join(save_diff_png, f"diff_{saved:03d}.png")
            os.makedirs(save_diff_png, exist_ok=True)
            plt.savefig(out_path, dpi=150, bbox_inches="tight")
            plt.close()
            saved += 1

    n = max(1, totals["count"])
    print("\n=== In-hole evaluation ===")
    print(f"MAE in hole:       {totals['mae_in']/n:.6f}")
    print(f"MAE outside hole:  {totals['mae_out']/n:.6f}  (should be near zero)")
    print(f"SI SDR in hole, model:    {totals['sdr_in_model']/n:.3f} dB")
    print(f"SI SDR in hole, baseline: {totals['sdr_in_baseline']/n:.3f} dB")
    print(f"Delta SDR (model - baseline): {(totals['sdr_in_model'] - totals['sdr_in_baseline'])/n:.3f} dB")

state = torch.load(checkpoint_path, map_location=DEVICE)
sd = remap_state_dict_keys(state["model_state_dict"])  # remove _orig_mod
model.load_state_dict(sd, strict=True)

print(next(iter(sd)).split('.')[0])  # see if keys start with _orig_mod
print("Restored mean diff:", torch.mean(torch.abs(restored - x_masked_stereo)).item())
print("Mask mean:", mask.mean().item())

eval_inhole_metrics(model, val_loader, device=DEVICE,
                    save_diff_png=os.path.join(CKPT_DIR, "diff_pngs"),
                    limit=8)