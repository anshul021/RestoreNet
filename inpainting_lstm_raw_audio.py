# -*- coding: utf-8 -*-
"""inpainting_LSTM_raw_audio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tia3t6ycXrpI6U6w6UixAp-Y7JG-tcFg
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
DS_FACTORS = (4, 4)     # total stride 16
LSTM_HIDDEN = 256
LSTM_LAYERS = 3
BIDIRECTIONAL = True
USE_RESIDUAL_COMPOSITION = True
SAMPLE_RATE = 44100
BASE_CHANNELS = 32
WIN_LEN = 65536 # previously 262144
MILLISECONDS_TO_SECONDS = 1e-3
MIN_HOLE_LENGTH = 5*MILLISECONDS_TO_SECONDS*SAMPLE_RATE
MAX_HOLE_LENGTH = 10*MILLISECONDS_TO_SECONDS*SAMPLE_RATE
BATCH_SZ = 8 # see if 16 works
NUM_WORKERS = 0 # previously 2, changed to 0 for debugging
start_LR = 3e-3
end_LR = 1e-4
WD = 1e-5 # previously 1e-3
DROPOUT = 0.2
accumulation_steps = 4
MAX_STEPS = 20000
VAL_EVERY = 250
INPUT_DIR = "/content/data"
CKPT_DIR = "drive/MyDrive/Checkpoints"
os.makedirs(CKPT_DIR, exist_ok=True)

print(len(os.listdir("/content/drive/MyDrive/Clean Audio")))
print(os.path.getsize("/content/drive/MyDrive/Clean Audio/10_10_35_42.wav.npy"))

import shutil
from tqdm.notebook import tqdm
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor

src_dir = "/content/drive/MyDrive/Clean Audio"
dst_dir = "/content/data"
os.makedirs(dst_dir, exist_ok=True)

TIMEOUT = 20  # seconds

def _copy_task(src, dst_dir):
    dst = os.path.join(dst_dir, os.path.basename(src))
    if not os.path.exists(dst):
        shutil.copy(src, dst)
    return dst

def copy_file(src):
    print("Copying", src)
    for attempt in range(1, 3):  # Try twice
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_copy_task, src, dst_dir)
            try:
                result = future.result(timeout=TIMEOUT)
                print(f"Successfully copied {src} on attempt {attempt}")
                return result
            except concurrent.futures.TimeoutError:
                print(f"Attempt {attempt}: Copying {src} exceeded {TIMEOUT} seconds")
    print(f"Failed to copy {src} after 2 attempts.")

print(os.listdir(src_dir)[:5])

len(os.listdir(dst_dir))

def clear_data():
    for item in os.listdir("/content/data"):
        item_path = os.path.join("/content/data", item)
        if os.path.isfile(item_path):
            os.remove(item_path)
        elif os.path.isdir(item_path):
            shutil.rmtree(item_path)

clear_data()

len(os.listdir(dst_dir))

num_files = 5

import random
random.seed(42)
files = [os.path.join(src_dir, f) for f in os.listdir(src_dir) if f.endswith(".npy")]
random_files = random.sample(files, num_files)

random_files[:5]

with ThreadPoolExecutor(max_workers=8) as executor:
    list(tqdm(executor.map(copy_file, files), total=len(files)))

import os
print(os.listdir("/content/data/"))
print(len(os.listdir("/content/data")))

import numpy as np

data = np.load('/content/data/13_5_18_24.wav.npy')

print(type(data))      # usually <class 'numpy.ndarray'>
print(data.shape)      # see the array dimensions
print(data.dtype)      # see the element type

print(data[0][:5])
print(data[1][:5])
print(max(data[0]))
print(max(data[1]))
print(min(data[0]))
print(min(data[1]))

del data
print("The 'data' variable has been deleted.")

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

def _norm(nc: int):
    # GroupNorm is stable for small batch sizes
    groups = max(1, nc // 8)
    return nn.GroupNorm(groups, nc)

_norm(120)

import os, math, random, glob
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from contextlib import nullcontext
from torch.cuda.amp import autocast, GradScaler
from tqdm.auto import tqdm
from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR

USE_CUDA = torch.cuda.is_available()
DEVICE = 'cuda' if USE_CUDA else 'cpu' # if on GPU

amp_ctx = torch.amp.autocast('cuda') if USE_CUDA else nullcontext()
scaler  = GradScaler(enabled=USE_CUDA)

import torch, os, subprocess, textwrap
print("torch.cuda.is_available():", torch.cuda.is_available())
print("torch.version.cuda:", torch.version.cuda)
print("CUDA devices:", torch.cuda.device_count())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

try:
    print(subprocess.check_output(["nvidia-smi"], text=True))
except Exception as e:
    print("nvidia-smi not available:", e)

USE_CUDA = torch.cuda.is_available()
assert USE_CUDA, "CUDA not available. Switch runtime to GPU."

import torch
import torch.nn as nn
import torch.nn.functional as F

class AudioInpaintLSTM(nn.Module):
    """
    LSTM-based inpainting model with light conv downsampling and upsampling.

    Input:
      x_masked_stereo: [B, 2, T]
      mask:            [B, 1, T]
    Returns:
      restored: [B, 2, T]
      residual: [B, 2, T]
    """
    def __init__(
        self,
        in_ch: int = 3,            # [L, R, mask]
        out_ch: int = 2,           # residual [L, R]
        base_ch: int = 64,
        ds_factors=(4, 4),         # total stride 16 (good for T=65536)
        lstm_hidden: int = 256,
        lstm_layers: int = 3,
        bidirectional: bool = True,
        dropout: float = 0.2,
        use_residual_composition: bool = True,
    ):
        super().__init__()
        self.use_residual_composition = use_residual_composition

        # Encoder: Conv1d with strides to reduce sequence length
        downs, ch = [], in_ch
        self.total_stride = 1
        for i, s in enumerate(ds_factors):
            outc = base_ch * (2 ** i)
            downs.append(nn.Sequential(
                nn.Conv1d(ch, outc, kernel_size=9, stride=s, padding=4),
                nn.GroupNorm(max(1, outc // 8), outc),
                nn.PReLU(outc),
                nn.Dropout1d(dropout),
            ))
            ch = outc
            self.total_stride *= s
        self.downs = nn.ModuleList(downs)

        # Temporal core: LSTM across time on the downsampled sequence
        self.bilstm = nn.LSTM(
            input_size=ch,
            hidden_size=lstm_hidden,
            num_layers=lstm_layers,
            dropout=dropout,
            bidirectional=bidirectional,
            batch_first=True,
        )
        lstm_out = lstm_hidden * (2 if bidirectional else 1)
        self.proj = nn.Linear(lstm_out, ch)

        # Decoder: transpose convs to return to original length
        ups = []
        for i, s in enumerate(reversed(ds_factors)):
            outc = base_ch * (2 ** (len(ds_factors) - i - 2)) if i < len(ds_factors) - 1 else base_ch // 2 if len(ds_factors) > 1 else base_ch
            # keep channels reasonable on the way up
            outc = max(outc, 32)
            ups.append(nn.Sequential(
                nn.ConvTranspose1d(ch, outc, kernel_size=9, stride=s, padding=4, output_padding=s - 1),
                nn.GroupNorm(max(1, outc // 8), outc),
                nn.PReLU(outc),
            ))
            ch = outc
        self.ups = nn.ModuleList(ups)

        # Final projection to stereo residual
        self.out = nn.Conv1d(ch, out_ch, kernel_size=3, padding=1)
        nn.init.zeros_(self.out.weight)
        nn.init.zeros_(self.out.bias)

    def forward(self, x_masked_stereo: torch.Tensor, mask: torch.Tensor):
        assert x_masked_stereo.ndim == 3 and mask.ndim == 3
        assert x_masked_stereo.size(1) == 2 and mask.size(1) == 1

        B, _, T = x_masked_stereo.shape
        x = torch.cat([x_masked_stereo, mask], dim=1)  # [B, 3, T]

        # pad to a multiple of total_stride
        pad = (self.total_stride - (T % self.total_stride)) % self.total_stride
        if pad:
            x = F.pad(x, (0, pad))

        # downsample
        for down in self.downs:
            x = down(x)  # [B, C, T']

        # LSTM across time
        x = x.transpose(1, 2)      # [B, T', C]
        y, _ = self.bilstm(x)      # [B, T', H*dirs]
        y = self.proj(y)           # [B, T', C]
        y = y.transpose(1, 2)      # [B, C, T']

        # upsample
        for up in self.ups:
            y = up(y)              # increasing time resolution

        # crop back to original T
        if y.size(-1) != T:
            y = y[..., :T]

        residual = self.out(y)     # [B, 2, T]

        if self.use_residual_composition:
            m2 = mask.repeat(1, 2, 1)
            restored = x_masked_stereo + residual * m2
        else:
            restored = residual

        return restored, residual

def masked_l1(restored, gt, mask):
    """
    restored, gt: [B, 2, T]
    mask:        [B, 1, T]  with 1s inside the hole, 0 elsewhere
    """
    m2 = mask.repeat(1, 2, 1)
    err = (restored - gt).abs() * m2
    return err.sum() / (m2.sum() + 1e-8)

def edge_ring(mask, width=256):
    """
    mask:  [B, 1, T] (binary 0/1)
    Returns a boundary ring mask [B, 1, T] of 0/1 near transitions of mask.
    """
    B, _, T = mask.shape
    # edge indicator: 1 where mask changes between adjacent samples
    diff = F.pad(mask[:, :, 1:] != mask[:, :, :-1], (1, 0), value=False).float()  # [B,1,T]
    # spread edges by 'width' using conv with ones
    k = 2*width + 1
    w = torch.ones(1, 1, k, device=mask.device)
    ring = F.conv1d(diff, w, padding=width)
    ring = (ring > 0).float()
    return ring

def boundary_l1(restored, gt, mask, width=256):
    ring = edge_ring(mask, width=width)
    r2 = ring.repeat(1, 2, 1)
    err = (restored - gt).abs() * r2
    return err.sum() / (r2.sum() + 1e-8)

def si_sdr_loss(restored, gt, weight=None, eps=1e-8):
    """
    restored, gt: [B, 2, T]
    weight:       [B, 1, T] or None (if None, uses uniform weighting)
    Returns: negative SI-SDR (mean over batch & channels)
    """
    if weight is None:
        weight = torch.ones_like(gt[:, :1, :])

    # expand weight to stereo
    w = weight.repeat(1, 2, 1)
    x = restored
    s = gt

    # weighted mean removal (scale-invariant works best with zero-mean)
    def wmean(z, w):
        return (z * w).sum(-1, keepdim=True) / (w.sum(-1, keepdim=True) + eps)

    x = x - wmean(x, w)
    s = s - wmean(s, w)

    # weighted dot products
    def wdot(a, b, w):
        return (a * b * w).sum(-1, keepdim=True)

    s_target_scale = wdot(x, s, w) / (wdot(s, s, w) + eps)
    s_target = s_target_scale * s
    e_noise = x - s_target

    num = (s_target**2 * w).sum(-1)
    den = (e_noise**2 * w).sum(-1)
    si_sdr = 10.0 * torch.log10((num + eps) / (den + eps))  # [B, 2]

    # loss is negative SI-SDR (maximize SI-SDR)
    return -(si_sdr.mean())

BOUNDARY_WIDTH = 512
W_L1_HOLE   = 1.0 # previously 0.0
W_L1_TRANSITION  = 0.1 # previously 0.25
W_SI_SDR   = 0.0 # previously 1.0

def total_loss(restored, gt, mask, ramp_steps=3000):
    # Optional warmup for SI-SDR (stabilizes early training)

    l_gap  = masked_l1(restored, gt, mask)
    l_seam = boundary_l1(restored, gt, mask, width=BOUNDARY_WIDTH)
    # Focus SI-SDR on the hole; include rim if you like:
    # rim = edge_ring(mask, width=BOUNDARY_WIDTH)
    # l_sdr = si_sdr_loss(restored, gt, weight=torch.clamp(mask + 0.25*rim, 0, 1))
    l_sdr  = si_sdr_loss(restored, gt, weight=mask)

    loss = W_L1_HOLE*l_gap + W_L1_TRANSITION*l_seam + W_SI_SDR*l_sdr
    logs = {"l_gap": float(l_gap.detach().cpu()),
            "l_seam": float(l_seam.detach().cpu()),
            "l_sdr": float(l_sdr.detach().cpu())}
    return loss, logs

@torch.no_grad()
def sanity_checks(gt, loss_fn):
    # shapes: [B, C, T]

    B, C, T = gt.shape

    start = T // 4
    end = 3 * T // 4
    size = end-start
    mask = torch.zeros((1, 1, T), device=gt.device)
    mask[:, :, start:end] = 1

    zeroes = zeroes = torch.zeros_like(gt)

    half_gt = gt/2

    noise = torch.empty_like(gt).uniform_(gt.min(), gt.max())

    variance_scale = 0.001  # smaller = milder
    moderate_noise = gt + torch.randn_like(gt) * variance_scale

    print("", start, end, start, "+", start + size // 4, start + size * 3 // 4, "+", end)

    low_quality_seam = gt.clone()

    # Fill first seam region with random values in [min, max]
    low_quality_seam[:, :, start:start+size//4] = torch.rand_like(
        low_quality_seam[:, :, start:start+size//4]
    ) * (gt.max() - gt.min()) + gt.min()

    # Fill second seam region with either a constant or random values
    low_quality_seam[:, :, start+size*3//4:end] = torch.rand_like(
        low_quality_seam[:, :, start+size*3//4:end]
    ) * (gt.max() - gt.min()) + gt.min()

    high_quality_seam = gt.clone()

    # Fill first seam region with random values in [min, max]
    high_quality_seam[:, :, start+size//4:start+size*3//4] = torch.rand_like(
        high_quality_seam[:, :, start+size//4:start+size*3//4]
    ) * (gt.max() - gt.min()) + gt.min()

    cases = {
        "perfect_gt": gt,
        "zeroes": zeroes,
        "half_gt": half_gt,
        "noise": noise,
        "moderate_noise": moderate_noise,
        "low_quality_seam": low_quality_seam,
        "high_quality_seam": high_quality_seam,
    }

    results = {}
    for name, y in cases.items():
        total, logs = loss_fn(y, gt, mask)
        results[name] = {"total": float(total), **{k: float(v) for k,v in logs.items()}}
    return results

test_gt = torch.from_numpy(np.load("/content/data/10_7_24_30.wav.npy")[:, 0:SAMPLE_RATE]).unsqueeze(0)
print("Input shape", test_gt.shape)
sanity_checks(test_gt, total_loss)

import os
import glob
import numpy as np
import json
from tqdm.auto import tqdm

# Path to your audio files
audio_dir = INPUT_DIR
all_audio_files = glob.glob(os.path.join(audio_dir, '*.npy'))

# Define the path for the lengths file
lengths_file = os.path.join(INPUT_DIR, 'audio_lengths.json')

# Check if the lengths file already exists
if os.path.exists(lengths_file):
    print(f"Lengths file already exists at {lengths_file}. Loading lengths.")
    with open(lengths_file, 'r') as f:
        audio_lengths = json.load(f)
else:
    print(f"Lengths file not found at {lengths_file}. Computing and saving lengths.")
    audio_lengths = {}
    # Iterate through all audio files and get their lengths
    for f in tqdm(all_audio_files, desc="Computing Audio Lengths"):
        try:
            # Use mmap_mode='r' to read shape without loading full data into memory
            length = np.load(f, mmap_mode='r').shape[-1]
            audio_lengths[f] = length
        except Exception as e:
            print(f"Error processing file {f}: {e}. Skipping.")

    # Save the computed lengths to a JSON file
    with open(lengths_file, 'w') as f:
        json.dump(audio_lengths, f)

    print(f"Audio lengths computed and saved to {lengths_file}")

# Now 'audio_lengths' dictionary contains {file_path: length} for all processed files
print(f"Loaded/Computed lengths for {len(audio_lengths)} audio files.")

import os
import glob
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split # Use sklearn for splitting

# Path to your audio files
audio_dir = INPUT_DIR
all_audio_files = glob.glob(os.path.join(audio_dir, '*.npy'))

# 1. Identify Unique Sources
# Extract the source ID (part before the first underscore) from each filename
source_ids = [os.path.basename(f).split('_')[0] for f in all_audio_files]
unique_source_ids = list(set(source_ids))

print(f"Found {len(all_audio_files)} total audio files.")
print(f"Found {len(unique_source_ids)} unique audio sources.")

# 2. Split Sources into Training and Validation Sets
# Use stratify if you have multiple files per source and want balanced splits
# For simplicity, random split of unique sources:
train_sources, val_sources = train_test_split(unique_source_ids, test_size=0.2, random_state=42) # Add random_state for reproducibility

print(f"Training sources: {len(train_sources)}")
print(f"Validation sources: {len(val_sources)}")

# Create lists of files belonging to the train and validation sources
train_files = [f for f in all_audio_files if os.path.basename(f).split('_')[0] in train_sources]
val_files = [f for f in all_audio_files if os.path.basename(f).split('_')[0] in val_sources]

print(f"Training files: {len(train_files)}")
print(f"Validation files: {len(val_files)}")

print([f for f in train_files if audio_lengths.get(f, 0) >= WIN_LEN])

import os
import numpy as np
import torch
from torch.utils.data import Dataset
import random
import json
from torch.utils.data import get_worker_info

class AudioInpaintingDataset(Dataset):
    def __init__(self, audio_files, win_len=WIN_LEN, audio_lengths=None):
        self.win_len = win_len
        # Filter files shorter than win_len immediately during initialization
        # Use the pre-computed audio_lengths dictionary if provided
        self.audio_files = [f for f in audio_files if audio_lengths.get(f, 0) >= self.win_len]
        print(f"Initialized dataset with {len(self.audio_files)} files meeting minimum length using pre-computed lengths.")


    def __len__(self):
        return len(self.audio_files)

    def __getitem__(self, idx):
        info = get_worker_info()
        if info is not None and idx == 0:  # print once per worker at start
            print(f"DataLoader worker {info.id} of {info.num_workers}")
        # Load an audio file by index
        audio_path = self.audio_files[idx]
        try:
            # Use mmap_mode='r' to avoid loading the whole file into memory at once if possible,
            # though numpy slicing will still load the segment.
            # Load the full file, but only for the selected segment, and copy it.
            # Loading the segment with mmap='r' and then copying ensures we only load
            # the necessary part into a writable numpy array before converting to torch.
            audio_data_mmap = np.load(audio_path, mmap_mode='r')

            # --- Data Preparation ---
            T = audio_data_mmap.shape[-1] # Total length of the audio

            # Select a random start position for the segment
            # Ensure there's enough space for the window (win_len)
            max_start_idx = T - self.win_len
            start_idx = np.random.randint(0, max_start_idx + 1)
            segment = audio_data_mmap[:, start_idx:start_idx + self.win_len].copy() # [2, win_len] - .copy() makes it writable
            gt = torch.from_numpy(segment).float() # Ground truth - now from a writable array

            # Create a mask with a random hole within the segment
            mask = torch.zeros(1, self.win_len) # [1, win_len]

            # Determine random start and end for the hole within the segment
            # Ensure hole_end is at least hole_start + 1 and within win_len
            hole_length = np.random.randint(MIN_HOLE_LENGTH, MAX_HOLE_LENGTH) # Keep hole length consistent

            # Calculate the bounds for the hole start to avoid first/last 20%
            safe_zone = int(self.win_len * 0.2)
            # Hole start must be after the first safe_zone samples
            # Hole end (hole_start + hole_length) must be before the last safe_zone samples
            min_hole_start = safe_zone
            max_hole_start = self.win_len - safe_zone - hole_length

            # Ensure max_hole_start is not less than min_hole_start (in case safe_zone is too large)
            if max_hole_start < min_hole_start:
                # If the window is too small to have a hole outside the safe zones,
                # throw an error
                raise Exception("The audio needs to be large enough for a hole to be able to be included without overlapping with the safe zones")
            else:
                hole_start_in_segment = np.random.randint(min_hole_start, max_hole_start + 1)


            mask[:, hole_start_in_segment:hole_start_in_segment + hole_length] = 1.0

            # Create the masked input with scaling instead of zeros
            m2 = mask.repeat(2, 1)  # [2, T]

            hole_end = hole_start_in_segment + hole_length

            # Start from clean audio, then modify only the masked region
            rng = random.Random()
            x_masked_stereo = gt.clone()  # [2, win_len]
            x_masked_stereo[:, hole_start_in_segment:hole_end] = rng.uniform(0.5, 2.0) * gt[:, hole_start_in_segment:hole_end]

            # Optional: keep inputs in range if your pipeline expects [-1, 1]
            x_masked_stereo = x_masked_stereo.clamp(-1.0, 1.0)

            return x_masked_stereo, mask, gt

        except Exception as e:
            print(f"Error loading or processing file {audio_path}: {e}")
            # Re-raise the exception to be caught by the DataLoader worker.
            # For robust training with potential bad files, a custom collate_fn returning None
            # and filtering None batches is better, but more complex to modify in place.
            raise e

import os
import glob
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split # Use sklearn for splitting

# Path to your audio files
audio_dir = INPUT_DIR
all_audio_files = glob.glob(os.path.join(audio_dir, '*.npy'))

# 1. Identify Unique Sources
# Extract the source ID (part before the first underscore) from each filename
source_ids = [os.path.basename(f).split('_')[0] for f in all_audio_files]
unique_source_ids = list(set(source_ids))

print(f"Found {len(all_audio_files)} total audio files.")
print(f"Found {len(unique_source_ids)} unique audio sources.")

# 2. Split Sources into Training and Validation Sets
# Use stratify if you have multiple files per source and want balanced splits
# For simplicity, random split of unique sources:
train_sources, val_sources = train_test_split(unique_source_ids, test_size=0.2, random_state=42) # Add random_state for reproducibility

print(f"Training sources: {len(train_sources)}")
print(f"Validation sources: {len(val_sources)}")

# Create lists of files belonging to the train and validation sources
train_files = [f for f in all_audio_files if os.path.basename(f).split('_')[0] in train_sources]
val_files = [f for f in all_audio_files if os.path.basename(f).split('_')[0] in val_sources]

print(f"Training files: {len(train_files)}")
print(f"Validation files: {len(val_files)}")
print(audio_lengths)


# 3. Create Dataset Instances using the filtered file lists and pre-computed lengths
# The dataset __init__ will now filter by win_len using the provided audio_lengths
train_dataset = AudioInpaintingDataset(train_files, win_len=WIN_LEN, audio_lengths=audio_lengths)
val_dataset = AudioInpaintingDataset(val_files, win_len=WIN_LEN, audio_lengths=audio_lengths)


# 4. Create data loaders
# Consider setting num_workers to 0 initially if you faced OSErrors with Drive
train_loader = DataLoader(
    train_dataset, batch_size=BATCH_SZ, shuffle=True,
    num_workers=NUM_WORKERS, persistent_workers=False, # persistent_workers=True when NUM_WORKERS = 2
    pin_memory=True, prefetch_factor=None # prefetch_factor=4 when NUM_WORKERS = 2
)
val_loader = DataLoader(
    val_dataset, batch_size=BATCH_SZ, shuffle=False,
    num_workers=NUM_WORKERS, persistent_workers=False, # persistent_workers=True when NUM_WORKERS = 2
    pin_memory=True, prefetch_factor=None # prefetch_factor=4 when NUM_WORKERS = 2
)


print(f"\nTraining set size (after win_len filter): {len(train_dataset)}")
print(f"Validation set size (after win_len filter): {len(val_dataset)}")
print(f"Training DataLoader created with batch size {BATCH_SZ} and num_workers={train_loader.num_workers}")
print(f"Validation DataLoader created with batch size {BATCH_SZ} and num_workers={val_loader.num_workers}")

import random

# Get a random index from the training dataset
random_index = random.randint(0, len(train_dataset) - 1)

# Get the sample using the random index
x_masked_stereo_sample, mask_sample, gt_sample = train_dataset[random_index]

print(f"Shape of masked stereo audio sample: {x_masked_stereo_sample.shape}")
print(f"Shape of mask sample: {mask_sample.shape}")
print(f"Shape of ground truth sample: {gt_sample.shape}")

print("Train size:", len(train_dataset), "Val size:", len(val_dataset))
assert len(train_dataset) > 0 and len(val_dataset) > 0

import math, torch

model = torch.compile(AudioInpaintLSTM(
    in_ch=3, out_ch=2,
    base_ch=BASE_CHANNELS,
    ds_factors=DS_FACTORS,
    lstm_hidden=LSTM_HIDDEN,
    lstm_layers=LSTM_LAYERS,
    bidirectional=BIDIRECTIONAL,
    dropout=DROPOUT,
    use_residual_composition=USE_RESIDUAL_COMPOSITION,
)).to(DEVICE)
model.train()

# Grab one batch
xb, mb, yb = next(iter(train_loader))
print("Batch shapes (CPU):", xb.shape, mb.shape, yb.shape)

xb = xb.to(DEVICE, non_blocking=True)
mb = mb.to(DEVICE, non_blocking=True)
yb = yb.to(DEVICE, non_blocking=True)

print("Batch shapes (GPU):", xb.shape, mb.shape, yb.shape)

# Forward once
with amp_ctx:
    out, res = model(xb, mb)
    loss, logs = total_loss(out, yb, mb)

print("Forward OK. out:", out.shape, "res:", res.shape, "loss:", loss.item())

# ONLY RUN IF CLEARING THE MODEL

import gc
import torch

def clear_memory():
    # Clear CUDA cache
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("CUDA cache cleared.")

    global model, optimizer, scaler

    # Delete model, optimizer, and scaler variables if they exist
    if 'model' in globals():
        del model
        print("'model' deleted.")
    if 'optimizer' in globals():
        del optimizer
        print("'optimizer' deleted.")
    if 'scaler' in globals():
        del scaler
        print("'scaler' deleted.")

    torch.cuda.empty_cache()

    # Run garbage collection
    gc.collect()
    print("Garbage collection complete.")

clear_memory()

import os, glob, itertools, torch
from tqdm import tqdm
import torch.profiler as profiler
from torch.cuda.amp import GradScaler

model = torch.compile(AudioInpaintLSTM(
    in_ch=3, out_ch=2,
    base_ch=BASE_CHANNELS,
    ds_factors=DS_FACTORS,
    lstm_hidden=LSTM_HIDDEN,
    lstm_layers=LSTM_LAYERS,
    bidirectional=BIDIRECTIONAL,
    dropout=DROPOUT,
    use_residual_composition=USE_RESIDUAL_COMPOSITION,
)).to(DEVICE)
optimizer = torch.optim.AdamW(model.parameters(), lr=start_LR, weight_decay=WD)
scaler = GradScaler(enabled=USE_CUDA)

total_logical_steps = MAX_STEPS
warmup_steps = max(1, int(0.05 * total_logical_steps))

warmup = LinearLR(
    optimizer,
    start_factor=1e-3,   # start at 0.001x of start_LR
    end_factor=1.0,
    total_iters=warmup_steps
)

cosine = CosineAnnealingLR(
    optimizer,
    T_max=total_logical_steps - warmup_steps,
    eta_min=end_LR
)

scheduler = SequentialLR(
    optimizer,
    schedulers=[warmup, cosine],
    milestones=[warmup_steps]
)

num_logical_steps = 10
profile_dir = "/content/profiling_results"
os.makedirs(profile_dir, exist_ok=True)
initial_files = set(glob.glob(os.path.join(profile_dir, "*")))

num_microsteps = num_logical_steps * accumulation_steps  # bars you expect

prof = profiler.profile(
    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],
    schedule=profiler.schedule(wait=0, warmup=1, active=num_microsteps, repeat=1),
    on_trace_ready=profiler.tensorboard_trace_handler(profile_dir),
    record_shapes=False,
    with_stack=False,
)

data_iter = itertools.cycle(train_loader)
model.train()
optimizer.zero_grad(set_to_none=True)

with prof:
    for t in tqdm(range(num_logical_steps), desc="Logical steps"):
        for k in range(accumulation_steps):
            batch = next(data_iter)
            if batch is None:
                print("BATCH DOES NOT EXIST")
                continue
            x_masked_stereo, mask, gt = batch
            x_masked_stereo = x_masked_stereo.to(DEVICE, non_blocking=True)
            mask = mask.to(DEVICE, non_blocking=True)
            gt = gt.to(DEVICE, non_blocking=True)

            with amp_ctx:
                restored, residual = model(x_masked_stereo, mask)
                loss, logs = total_loss(restored, gt, mask)

            scaler.scale(loss / accumulation_steps).backward()

            # one bar per microstep
            torch.cuda.synchronize()       # optional but makes timings cleaner
            prof.step()

        scaler.step(optimizer)             # once per logical step
        scaler.update()
        scheduler.step()
        optimizer.zero_grad(set_to_none=True)

print(f"Profiling finished. Results saved to {profile_dir}")

final_files = set(glob.glob(os.path.join(profile_dir, "*")))
new_files = list(final_files - initial_files)
print("\nGenerated profiling trace files:")
print("\n".join(f"- {os.path.basename(f)}" for f in new_files) or "No new trace files were generated during this run.")
print("\nYou can analyze the results using TensorBoard.")

print("3d8340cadb11_5660.1761114826290839016.pt.trace.json" in os.listdir("/content/profiling_results"))
print(os.path.join("/content/profiling_results", "3d8340cadb11_5660.1761114826290839016.pt.trace.json"))

!pip install torch-tb-profiler

os.makedirs("/content/drive/MyDrive/Profiling_Results/V11", exist_ok=True)
!cp -r "/content/profiling_results/3d8340cadb11_5660.1761114826290839016.pt.trace.json" "/content/drive/MyDrive/Profiling_Results/V11"

!ls -R "/content/drive/MyDrive/Profiling_Results/V11"

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir "/content/drive/MyDrive/Profiling_Results/V6_4"

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir "/content/drive/MyDrive/Profiling_Results/V11"

print(train_files[0:10])
print("_____________________________________________")
print(val_files[0:10])

print(len(train_loader))

train_losses = []
val_losses = []

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb:128"
# Optional, avoids heavy autotune workspaces that can spike memory
os.environ["TORCHINDUCTOR_MAX_AUTOTUNE_GEMM"] = "0"

from tqdm import tqdm

# Initialize model, optimizer, and scaler
import itertools, time, os, torch
from torch.cuda.amp import GradScaler

model = torch.compile(AudioInpaintLSTM(
    in_ch=3, out_ch=2,
    base_ch=BASE_CHANNELS,
    ds_factors=DS_FACTORS,
    lstm_hidden=LSTM_HIDDEN,
    lstm_layers=LSTM_LAYERS,
    bidirectional=BIDIRECTIONAL,
    dropout=DROPOUT,
    use_residual_composition=USE_RESIDUAL_COMPOSITION,
)).to(DEVICE)
optimizer = torch.optim.AdamW(model.parameters(), lr=start_LR, weight_decay=WD)
scaler = GradScaler(enabled=USE_CUDA)

total_logical_steps = MAX_STEPS
warmup_steps = max(1, int(0.05 * total_logical_steps))

warmup = LinearLR(
    optimizer,
    start_factor=1e-3,   # start at 0.001x of start_LR
    end_factor=1.0,
    total_iters=warmup_steps
)

cosine = CosineAnnealingLR(
    optimizer,
    T_max=total_logical_steps - warmup_steps,
    eta_min=end_LR
)

scheduler = SequentialLR(
    optimizer,
    schedulers=[warmup, cosine],
    milestones=[warmup_steps]
)

print("Model, optimizer, and scaler initialized.")

# Training control
step = 0  # logical step counter
best_val_loss = float('inf')
print(f"Starting training on {DEVICE}")

# Timing
training_start_time_abs = time.time()
print(f"Training started at absolute time: {training_start_time_abs:.4f} seconds")

# Cycling data iterator like the profiling loop
data_iter = itertools.cycle(train_loader)

# Accumulators over the current validation window
total_train_loss = 0.0
train_logs = {"l_gap": 0.0, "l_seam": 0.0, "l_sdr": 0.0}
num_train_batches = 0

model.train()
optimizer.zero_grad(set_to_none=True)

# Create directory for loss logs
log_dir = "/content/errors"
os.makedirs(log_dir, exist_ok=True)
loss_log_path = os.path.join(log_dir, "loss.log")

num_val_loops = 5 # Define the number of times to loop through the validation loader

try:
    with tqdm(total=MAX_STEPS, initial=step, desc="Train steps", dynamic_ncols=True) as pbar:
        while step < MAX_STEPS:
            # One logical step equals accumulation_steps microsteps
            accumulation_start_time = time.time() - training_start_time_abs

            for k in range(accumulation_steps):
                x_masked_stereo, mask, gt = next(data_iter)
                x_masked_stereo = x_masked_stereo.to(DEVICE, non_blocking=True)
                mask = mask.to(DEVICE, non_blocking=True)
                gt = gt.to(DEVICE, non_blocking=True)

                with amp_ctx:
                    restored, residual = model(x_masked_stereo, mask)
                    loss, logs = total_loss(restored, gt, mask) # Removed step and ramp_steps here

                scaler.scale(loss / accumulation_steps).backward()

                total_train_loss += loss.item()
                train_logs["l_gap"] += logs["l_gap"]
                train_logs["l_seam"] += logs["l_seam"]
                train_logs["l_sdr"] += logs["l_sdr"]
                num_train_batches += 1

                if USE_CUDA:
                    torch.cuda.synchronize()

            # Optimizer step once per logical step
            accumulation_end_time = time.time() - training_start_time_abs
            accumulation_duration = accumulation_end_time - accumulation_start_time

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)
            scheduler.step()

            step += 1
            pbar.update(1)  # progress bar tick

            avg_train_loss = total_train_loss / num_train_batches
            avg_train_logs = {k: v / num_train_batches for k, v in train_logs.items()}
            train_losses.append({
                "step": step,
                "total_loss": avg_train_loss,
                "l_gap": avg_train_logs["l_gap"],
                "l_seam": avg_train_logs["l_seam"],
                "l_sdr": avg_train_logs["l_sdr"]
            })


            if step >= MAX_STEPS:
                break

            # Validation every VAL_EVERY logical steps
            if step % VAL_EVERY == 0:
                print("\n" + "=" * 50)
                print(f"=== Starting Validation at Step {step} ===")
                val_start_time = time.time() - training_start_time_abs

                model.eval()
                total_val_loss = 0.0
                val_logs = {"l_gap": 0.0, "l_seam": 0.0, "l_sdr": 0.0}
                num_val_batches = 0

                for val_loop_idx in range(num_val_loops): # Loop multiple times
                    print(f"  Validation Pass {val_loop_idx + 1}/{num_val_loops}")
                    for x_masked_stereo_val, mask_val, gt_val in tqdm(
                        val_loader, desc=f"Step {step}/{MAX_STEPS} (Val Pass {val_loop_idx + 1})", leave=False, dynamic_ncols=True
                    ):
                        x_masked_stereo_val = x_masked_stereo_val.to(DEVICE, non_blocking=True)
                        mask_val = mask_val.to(DEVICE, non_blocking=True)
                        gt_val = gt_val.to(DEVICE, non_blocking=True)

                        with amp_ctx:
                            restored_val, residual_val = model(x_masked_stereo_val, mask_val)
                            val_loss, logs_val = total_loss(restored_val, gt_val, mask_val)

                        total_val_loss += val_loss.item()
                        val_logs["l_gap"] += logs_val["l_gap"]
                        val_logs["l_seam"] += logs_val["l_seam"]
                        val_logs["l_sdr"] += logs_val["l_sdr"]
                        num_val_batches += 1

                val_end_time = time.time() - training_start_time_abs
                val_duration = val_end_time - val_start_time
                print(f"=== Finished Validation at Step {step} ===")
                print(
                    f"Validation | Start: {val_start_time:.4f}s | End: {val_end_time:.4f}s | "
                    f"Duration: {val_duration:.4f} seconds."
                )
                print("=" * 50 + "\n")
                print(f"step {step} lr {scheduler.get_last_lr()[0]:.6e}")
                print("=" * 50 + "\n")
                print("\n")

                # Report averages and update tqdm postfix
                avg_val_loss = total_val_loss / max(1, num_val_batches)
                avg_val_logs = {k: v / max(1, num_val_batches) for k, v in val_logs.items()}


                print(
                    f"Step {step}: Val Loss = {avg_val_loss:.4f}"
                )
                print(
                    "  Val Logs:   "
                    f"l_gap={avg_val_logs['l_gap']:.4f}, "
                    f"l_seam={avg_val_logs['l_seam']:.4f}, "
                    f"l_sdr={avg_val_logs['l_sdr']:.4f}"
                )

                # Append validation losses
                val_losses.append({
                    "step": step,
                    "total_loss": avg_val_loss,
                    "l_gap": avg_val_logs["l_gap"],
                    "l_seam": avg_val_logs["l_seam"],
                    "l_sdr": avg_val_logs["l_sdr"]
                })

                # Log the loss for this microstep
                with open(loss_log_path, "a") as f:
                    f.write(f"Step: {step}\n")
                    f.write(f"l_gap: {avg_val_logs['l_gap']}\n")
                    f.write(f"l_seam: {avg_val_logs['l_seam']}\n")
                    f.write(f"l_sdr: {avg_val_logs['l_sdr']}\n")
                    f.write("_"*50)


                # Checkpoint best
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    checkpoint_path = os.path.join(CKPT_DIR, f"best_model_step_{step}.pth")
                    torch.save(
                        {
                            "step": step,
                            "model_state_dict": model.state_dict(),
                            "optimizer_state_dict": optimizer.state_dict(),
                            "best_val_loss": best_val_loss,
                        },
                        checkpoint_path,
                    )
                    print(f"Saved best model checkpoint to {checkpoint_path}")


                model.train()

except Exception as e:
    print(f"An error occurred during training: {e}")

print("Training finished.")

def _to_stereo_int16(x: np.ndarray) -> np.ndarray:
    """Convert float32 [-1,1] array [2,T] to int16 [T,2] for WAV output."""
    if x.ndim == 2 and x.shape[0] == 2:
        x = x.T
    x = np.clip(x, -1.0, 1.0)
    return (x * 32767.0).astype(np.int16)

import os
import torch
import numpy as np
from tqdm import tqdm
from scipy.io import wavfile

SAVE_DIR = "/content/eval"
os.makedirs(SAVE_DIR, exist_ok=True)

@torch.no_grad()
def evaluate_and_save_audio(checkpoint_path, loader,
                            total_loss_fn, device='cuda',
                            sample_rate=48000, limit=None):
    print(f"Loading model from: {checkpoint_path}")
    model = torch.compile(AudioInpaintLSTM(
        in_ch=3, out_ch=2,
        base_ch=BASE_CHANNELS,
        ds_factors=DS_FACTORS,
        lstm_hidden=LSTM_HIDDEN,
        lstm_layers=LSTM_LAYERS,
        bidirectional=BIDIRECTIONAL,
        dropout=DROPOUT,
        use_residual_composition=USE_RESIDUAL_COMPOSITION,
    )).to(DEVICE)
    state = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(state["model_state_dict"])  # keys will match
    model.eval()


    total_loss = 0.0
    total_logs = {"l_gap": 0.0, "l_seam": 0.0, "l_sdr": 0.0}
    num_batches = 0
    saved = 0
    idx = 0

    for batch in tqdm(loader, desc="Evaluating"):
        x_masked_stereo, mask, gt = [t.to(device) for t in batch]
        restored, _ = model(x_masked_stereo, mask)
        loss, logs = total_loss_fn(restored, gt, mask)

        print("\n============================")
        print(loss)
        print("============================")

        total_loss += loss.item()
        for k in total_logs:
            total_logs[k] += logs[k]
        num_batches += 1

        # save audio
        x_masked_np = x_masked_stereo.cpu().numpy()
        restored_np = restored.cpu().numpy()
        gt_np = gt.cpu().numpy()

        B = x_masked_np.shape[0]
        for b in range(B):
            base = f"val_{idx:05d}"
            wavfile.write(os.path.join(SAVE_DIR, f"{base}_masked.wav"),
                          sample_rate, _to_stereo_int16(x_masked_np[b]))
            wavfile.write(os.path.join(SAVE_DIR, f"{base}_restored.wav"),
                          sample_rate, _to_stereo_int16(restored_np[b]))
            wavfile.write(os.path.join(SAVE_DIR, f"{base}_gt.wav"),
                          sample_rate, _to_stereo_int16(gt_np[b]))
            idx += 1
            saved += 3
        if limit is not None and idx >= limit:
            break

    avg_loss = total_loss / max(1, num_batches)
    avg_logs = {k: v / max(1, num_batches) for k, v in total_logs.items()}

    print(f"\n=== Evaluation Complete ===")
    print(f"Validation Loss: {avg_loss:.4f}")
    for k, v in avg_logs.items():
        print(f"{k}: {v:.4f}")
    print(f"Saved {saved} WAV files in {SAVE_DIR}")
    print("============================")

checkpoint_path = "/content/drive/MyDrive/Checkpoints/best_model_step_1500.pth"
print(os.path.exists(checkpoint_path))
evaluate_and_save_audio(checkpoint_path, val_loader,
                        total_loss, device=DEVICE, sample_rate=SAMPLE_RATE, limit=5)

import matplotlib.pyplot as plt

def plot_loss(x, train_y, val_y, loss_name):
    plt.figure()
    plt.plot(x, train_y, label='Training', marker='o')
    plt.plot(x, val_y, label='Validation', marker='o')
    plt.xlabel('Step')
    plt.ylabel(loss_name)
    plt.title(f'{loss_name} over Steps')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

steps = [x*250 for x in range(1, 69)]
train_total_loss = [step["total_loss"] for step in train_losses if step["step"]%250 == 0 and step["step"] != 20000]
val_total_loss = [step["total_loss"] for step in val_losses if step["step"]%250 == 0]

plot_loss(steps, train_total_loss, val_total_loss, "Total Loss")